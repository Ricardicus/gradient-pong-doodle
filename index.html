<!DOCTYPE html>
<html>
<head>
<title>Gradient ping pong doodle</title>
</head>
<body>
<div id="cdiv">
<h2>Gradient ping pong doodle</h2>
<pre>
The angle arsms are trying to reach the red ball and play
ping pong. It does so by attempting to find its correct angle values.
This process is done using gradient optimization schemes.
Sometimes it works, sometimes it doesn't. It is an open problem.
One can use different hyperparameters to help each of the arms along.
What optimization strategy will win?
</pre>
<div style="width=100%">
<canvas id ="tcanvas" style="font-family: 'Hammersmith One', sans-serif; border: 1px solid black; border-radius: 4px;"
	width="700">
Your browser does not support the HTML5 canvas tag.</canvas>
<h2 id="scoreboard"></h2>
</div>
</div>
<table>
<tbody>
<tr>
<td>
<div>
  <div id="arm-1-omega-bars">
  </div>
  <table>
    <tbody>
      <tr>
        <td>Loss: </td>
        <td><p id="arm-1-loss"></p></td>
      </tr>
    </tbody>
  </table>
<fieldset>
    <legend>Select optimization strategy:</legend>
    <div>
      <input onchange="newOptStrat(0);" type="radio" id="arm-1-gradient-descent-select" value="basic" name="arm0opt"
             checked>
      <label for="huey">Gradient Descent <a href="https://en.wikipedia.org/wiki/Gradient_descent">[link]</a></label>
    </div>
    <div>
      <input onchange="newOptStrat(0);" type="radio" id="arm-1-adam-select" value="adam" name="arm0opt">
      <label for="dewey">ADAM <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam">[link]</a></label>
    </div>
    <div>
      <input onchange="newOptStrat(0);" type="radio" id="arm-1-adagrad-select" value="adagrad" name="arm0opt">
      <label for="adagrad">AdaGrad <a href="https://optimization.cbe.cornell.edu/index.php?title=AdaGrad">[link]</a></label>
    </div>
</fieldset>

<input type="range" oninput="newLearningRate(0);"
         min="0" max="100" id="arm-1-learning-rate">
<label id="arm-1-label-learning-rate" for="arm-1-learning-rate">Learning rate</label><br/>
<p id="arm-1-debug"></p>
<br/>
<div id="arm-1-adam-inputs"> 
<input type="range" oninput="newADAMBeta1(0);"
         min="0" max="100" id="arm-1-adam-beta-1">
  <label id="arm-1-label-adam-beta-1" for="adam-beta-1">ADAM β1</label><br/>
<input type="range" oninput="newADAMBeta2(0);"
         min="0" max="100" id="arm-1-adam-beta-2">
  <label id="arm-1-label-adam-beta-2" for="adam-beta-2">ADAM β2</label><br/>
</div>
</td>
<td>


<div>
  <div id="arm-2-omega-bars">
  </div>
  <table>
    <tbody>
      <tr>
        <td>Loss: </td>
        <td><p id="arm-2-loss"></p></td>
      </tr>
    </tbody>
  </table>
<fieldset>
    <legend>Select optimization strategy:</legend>
    <div>
      <input onchange="newOptStrat(1);" type="radio" id="arm-2-gradient-descent-select" value="basic" name="arm1opt"
             checked>
      <label for="huey">Gradient Descent <a href="https://en.wikipedia.org/wiki/Gradient_descent">[link]</a></label>
    </div>
    <div>
      <input onchange="newOptStrat(1);" type="radio" id="arm-2-adam-select" value="adam" name="arm1opt">
      <label for="dewey">ADAM <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam">[link]</a></label>
    </div>
    <div>
      <input onchange="newOptStrat(1);" type="radio" id="arm-2-adagrad-select" value="adagrad" name="arm1opt">
      <label for="adagrad">AdaGrad <a href="https://optimization.cbe.cornell.edu/index.php?title=AdaGrad">[link]</a></label>
    </div>
</fieldset>

<input type="range" oninput="newLearningRate(1);"
         min="0" max="100" id="arm-2-learning-rate">
<label id="arm-2-label-learning-rate" for="arm-2-learning-rate">Learning rate</label><br/>
<p id="arm-2-debug"></p>
<br/>
<div id="arm-2-adam-inputs"> 
   <input type="range" oninput="newADAMBeta1(1);"
         min="0" max="100" id="arm-2-adam-beta-1">
  <label id="arm-2-label-adam-beta-1" for="arm-2-adam-beta-1">ADAM β1</label><br/>
<input type="range" oninput="newADAMBeta2(1);"
         min="0" max="100" id="arm-2-adam-beta-2">
  <label id="arm-2-label-adam-beta-2" for="arm-2-adam-beta-2">ADAM β2</label><br/>
</div>



</td>
</tr>
</tbody>
</table>
<br/>
<pre>
ADAM requires a higher value for the "learning rate" hyperparameter than the basic Gradient Descent.
So you typically need to raise it a bit after switching from Gradient Descent.
</pre>

<br/>
Link to source code: <a href"https://github.com/Ricardicus/gradient-pong-doodle">gradient-doodle</a>

</div>
</body>
<script src="gradient-pong-doodle.js">
</script>
</html>
